<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>blog - page 3 | Marvellous Oluwamayowa Ajala</title>
    <meta name="author" content="Marvellous Oluwamayowa Ajala" />
    <meta name="description" content="A pharmacist, ML engineer/researcher and research scientist in the making
" />
    <meta name="keywords" content="machine learning, drug discovery, healthcare, pharmacy" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://ajalamarvellous.github.io/blog/page/3/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Marvellous&nbsp;</span>Oluwamayowa&nbsp;Ajala</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <div class="post">

  
  

  
  <div class="header-bar">
    <h1>Blog</h1>
    <h2>a simple whitespace theme for academics</h2>
  </div>
  

  
  <div class="tag-category-list">
    <ul class="p-0 m-0">
      
        <li>
          <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a>
        </li>
        
          <p>&bull;</p>
        
      
        <li>
          <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a>
        </li>
        
          <p>&bull;</p>
        
      
        <li>
          <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a>
        </li>
        
          <p>&bull;</p>
        
      
        <li>
          <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a>
        </li>
        
          <p>&bull;</p>
        
      
        <li>
          <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a>
        </li>
        
      
      
        <p>&bull;</p>
      
      
        <li>
          <i class="fas fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a>
        </li>
        
      
    </ul>
  </div>
  

  <ul class="post-list">
    

    
    
    
    

    <li><h3>
        
          <a class="post-title" href="https://madeofajala.hashnode.dev/real-time-auto-conversion-of-your-notebooks-to-python-scripts-using-jupytext" target="_blank">REAL-TIME AUTO CONVERSION OF YOUR NOTEBOOKS TO PYTHON SCRIPTS USING JUPYTEXT</a>
          <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg">
            <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path>
          </svg>
        
      </h3>
      <p><h1 id="heading-introduction">Introduction</h1>
<p>Notebooks are essential tools for data scientists, machine learning engineers and researchers as they allow fast experimentation and also make sharing both the codes and results easier. But there lies one of the greatest challenges with using notebooks, version control is tough or like some people will call it, a mess, with notebooks.
Just in case the term version control is new to you, it is basically the process of keeping track and a record of all your codes (or whatever it is you want to version control) sequentially such that in case something bad or terrible happens e.g someone mistakenly deleting your code base (all your codes where you saved them), you have somewhere you can roll back to the last version you saved without you having to rewrite them again. It's like making a photocopy of your codes at every point during development so you can always have something to roll back on. You'll agree with me that is a great tool data scientists also need to adopt.</p>
<p>Back to notebooks, version control tools like git or subversion (depending on your choice) were designed primarily for human-readable texts. Notebooks, however, contain a large nested JSON (JavaScript Object Notation) file containing codes, markdowns, HTML and others all combined together, this means the average notebook is difficult to read using git or subversion (thereby making it hard to use so many capabilities embedded in git like allowing you to see modifications to your code since the last time you saved it among others)</p>
<blockquote>
<p>JSON (JavaScript Object Notation) files are a special file format that contains a list of dictionaries for storing and transporting data. Although originally designed for JavaScript, they are now a ubiquitous file format commonly used on the web for transferring and storing data.</p>
</blockquote>
<p>Apart from version controlling, notebooks also have some challenges that make them inferior to using just python scripts, this includes reproducibility of codes. Codes in notebooks can be run out of order, with the owner editing a part out after they've run it. This means someone trying to follow their steps will not get the same result as they did due to this edited part and this hinders reproducibility. Codes hosted in notebooks are also not callable by codes in an external location, this makes them impossible to be tested. </p>
<p>These and many others are some of the challenges with notebooks and why notebooks aren't used in production environments. The option however was to experiment in notebooks and then transfer the codes manually or rewrite them as scripts. This process as we can see is very inefficient and time-consuming, in this tutorial I'll be showing you how to automatically convert your notebooks to scripts using jupytext.</p>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/OBJfJhIr9E7Jq3ffNV/giphy.gif">https://media.giphy.com/media/OBJfJhIr9E7Jq3ffNV/giphy.gif</a></div>
<h3 id="heading-tool">Tool</h3>
<ul>
<li>jupytext</li>
<li>jupyter-notebok or jupyter-lab</li>
</ul>
<h2 id="heading-installation">Installation</h2>
<p>So the first thing we need to do is to download jupytext. Jupytext is a tool that automatically converts your notebooks into python scripts. To download jupytext, you need to use the package installer for python (pip).</p>
<pre><code class="lang-bash">    pip install jupytext
</code></pre>
<p>or using conda;</p>
<pre><code class="lang-bash">    conda install jupytext
</code></pre>
<h2 id="heading-setting-up-jupytext">Setting up Jupytext</h2>
<p>After downloading, now we are ready to use jupytext for the automatic conversion of notebooks to scripts. There are so many ways to set up/jupytext from here, however, I will be showing you 3 ways to use jupytext. But before we proceed, let me give a little brief on our project folder structure;</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657503127560/71RJYkq_O.png" alt="image.png" /></p>
<p>Our project has a folder "notebooks" where we will keep all our notebooks and another folder src, with different subfolders where we will keep our scripts. This is important because we will need to tell jupytext where to find our notebooks and where to keep our scripts when it is done. Now, let's get into it.</p>
<h3 id="heading-using-notebooks-metadata">Using notebook's metadata</h3>
<p>The first way we will be using to configure our jupytext is by editing our notebook's metadata. To do that open your jupyter notebook.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657499985784/6mhkOtHDx.png" alt="Screenshot from 2022-07-10 23-19-16.png" /></p>
<p>Next, we will create a new notebook [inside the notebooks folder] and call it Tryout.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500886458/pvsxfOC7a.png" alt="image.png" /></p>
<p>After that, we will click on the Edit on the menu bar and select "Edit Metadata";</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500294171/mCFb2gsSx.png" alt="Screenshot 2022-07-10 at 23-23-51 Tryout - Jupyter Notebook.png" /></p>
<p>You should have the Edit Notebook Metadata open like mine.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500367867/NElsV-LCk.png" alt="Screenshot 2022-07-10 at 23-24-29 Tryout - Jupyter Notebook.png" />
piture of open meta data page]</p>
<p>Then you should insert this into the box and select enter</p>
<pre><code>  <span class="hljs-string">"jupytext"</span>: {
    <span class="hljs-string">"formats"</span>: <span class="hljs-string">"notebooks///ipynb,src/data///py"</span>
  },
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500419574/WOohwzGUz.png" alt="Screenshot 2022-07-11 at 00-24-40 Tryout - Jupyter Notebook.png" /></p>
<p>Here, we are telling jupytext (acting in the background), that select all notebooks in this folder (that ends with ipynb), convert them to "py" scripts in ".../src/data" (or any of the subfolders in our src folder).</p>
<p>Now we can play around with our notebook and automatically whenever jupyter saves our notebook, a python script is generated with the exact name of our notebook and the same content.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500498804/G7HFCSn9S.png" alt="Screenshot 2022-07-11 at 00-32-41 Tryout - Jupyter Notebook.png" /></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657503613446/Ft9yQvZ4Z.png" alt="image.png" /></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500598462/7nb1mvqvR.png" alt="Screenshot from 2022-07-11 00-33-54.png" /></p>
<h2 id="heading-using-jupytext-configuration-file">Using jupytext configuration file</h2>
<p>Another way to set up jupytext with jupyter notebook is by creating a configuration file for it. jupytext accepts configuration files that end with .jupytext or .jupytext.toml or just jupytext.toml. After creating the file, we will then enter this into the file and save it.</p>
<pre><code class="lang-text">formats = "notebooks///ipynb,src/data///py"
</code></pre>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657500969226/pAqI1zIiY.png" alt="image.png" /></p>
<p>or </p>
<pre><code class="lang-text">[formats]
"notebooks/" = "ipynb"
"src/data" = "py"
</code></pre>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657501062372/e87vGI1iU.png" alt="image.png" /></p>
<p>They both do the same thing of telling jupytext where to find our notebooks and where to save our scripts and every time jupyter saves our notebook, we automatically get our scripts also.</p>
<h2 id="heading-using-pre-commit-hook">Using pre-commit hook</h2>
<p>The last method I'll be explaining to set up jupytext is using it with pre-commit hook. For those not familiar with pre-commit, pre-commit is a tool that ensures that before you commit (add and save) anything to git, certain conditions e.g ensuring the code is formatted properly, that there is no trailing space after each line, that every imported module is used etc are fulfilled, if not, the commit should not be accepted. You can therefore use pre-commit to enforce consistency for your codes.
You can also use jupytext alongside with the other hooks we want to set up for our codes.</p>
<p>To use a pre-commit hook for your notebooks, you need to download pre-commit, so we'll download pre-commit.</p>
<pre><code class="lang-bash">   pip install pre-commit
</code></pre>
<p>or using;</p>
<pre><code class="lang-bash">   conda install jupytext -c conda-forge
</code></pre>
<p>After that, we will need to create a .pre-commit-config.yaml file, this file will contain the configuration of all our pre-commit hooks. To add jupytext to .pre-commit-config.yaml file, add;</p>
<pre><code class="lang-yaml"><span class="hljs-attr">repos:</span>
<span class="hljs-bullet">-</span>   <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/mwouts/jupytext</span>
    <span class="hljs-attr">rev:</span> <span class="hljs-string">v1.14.0</span>  <span class="hljs-comment"># CURRENT_TAG/COMMIT_HASH</span>
    <span class="hljs-attr">hooks:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">id:</span> <span class="hljs-string">jupytext</span>
      <span class="hljs-attr">args:</span> [<span class="hljs-string">--sync</span>]
</code></pre>
<p>Here, we are telling pre-commit that we are about to add the jupytext hook, the repo represents the GitHub repository for the hook (jupytext), rev represents the tag (version of jupytext) we are using, and under the hook, we have the id of the hook (name) which is jupytext and an argument to synchronise our notebook with the scripts.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657501236520/QGjgwTGU6.png" alt="Screenshot 2022-07-11 at 01-07-12 mwouts_jupytext Jupyter Notebooks as Markdown Documents Julia Python or R scripts.png" /></p>
<p>Jupytext when combined with some other pre-commit hooks, mine looks like this, (yours should too).</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1657501309475/XTBXMriYR.png" alt="Screenshot from 2022-07-11 01-04-02.png" /></p>
<p>then on the terminal, run;</p>
<pre><code class="lang-bash">   pre-commit install
</code></pre>
<p>This downloads and set up pre-commit. After this every time, we add our notebooks to be committed to git, a synchronised script is generated for us. You can learn more about <a target="_blank" href="https://pre-commit.com/">pre-commit</a> or watch out for my next article on important pre-commit hooks you should be using as a data scientist and how to set it up</p>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/l0ExjbToOj4n23OmY/giphy.gif">https://media.giphy.com/media/l0ExjbToOj4n23OmY/giphy.gif</a></div>
<p>And that is a simple setup using jupytext, of how to automatically and in real-time, convert your notebooks to scripts. If you enjoy or found this article helpful, please like, subscribe, drop feedback and share with someone else you think might need it. Till next time, see you.</p>
</p>
      <p class="post-meta">
        1 min read &nbsp; &middot; &nbsp;
        July 11, 2022
        &nbsp; &middot; &nbsp; hashnode.com
      </p>
      <p class="post-tags">
        <a href="/blog/2022">
          <i class="fas fa-calendar fa-sm"></i> 2022 </a>

          

          
    </p></li>

    

    
    
    
    

    <li><h3>
        
          <a class="post-title" href="https://madeofajala.hashnode.dev/setting-up-a-data-science-project-folder" target="_blank">Setting Up A Data Science Project Folder</a>
          <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg">
            <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path>
          </svg>
        
      </h3>
      <p><p>Almost all data scientist start out their career with a project folder like this.
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872295017/s7utlOdAZ.png" alt="image.png" /></p>
<p>A single folder containing a single notebook <em>(or maybe two)</em> containing all their codes i.e for data ingestion, data transformation and preprocessing, model building, model inference and also model deployment <em>(its possible to deploy models from colab these days)</em> until they move into a proper work setting and now, most of their colleagues dont use notebooks, <em>what on earth are notebooks even?</em> So they have to split their notebook(s) into web scraping/ data ingestion script, data transformation and deployment scripts, model building script, model inference script and model deployment script. Now they also have to write tests for all their scripts, so test scripts too. They have to keep track of all their raw data files and the different preprocessed data files. They cant perform inference on the go, so they have to save their different models too plus the different charts (ROC values, data distribution) they have to share with their boss. Within a short duration of time, theyve moved to a folder like this.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872371640/8VjeRyPRj.png" alt="image.png" /></p>
<p>The chances however are, theres a high probability of you messing up something and thats something you definitely want to avoid. This means you need a proper project folder structure, but that is something you dont want to spend a lot of your time worrying about because youre yet even to start the project. In this tutorial, Ill show you how you can set up a proper folder structure for your data science project with minimal effort.</p>
<h4 id="heading-tools-well-be-using">Tools we'll be using</h4>
<ul>
<li>Poetry</li>
<li>Datasist</li>
<li>COokiecutter</li>
</ul>
<h3 id="heading-create-a-virtual-environment">Create a virtual environment</h3>
<p>First, lets create a virtual environment to isolate this new project from the existing dependencies on our PC. This helps to ensure the reproducibility of our code and prevent conflicting versions of any dependency or package that may break our code. To do that, lets create a new folder where our project will be and the virtual environment will be inside.</p>
<pre><code>mkdir <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p>we created a new folder named it new-project.</p>
<pre><code>python3 <span class="hljs-operator">-</span> m venv <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project<span class="hljs-operator">/</span>venv
</code></pre><p>Next, we created a virtual environment inside this folder</p>
<pre><code>cd <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p>Then we opened the folder</p>
<pre><code>ls
</code></pre><p>We list the constituents of the folder</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872399217/r1Yq_yKkw.png" alt="image.png" /></p>
<p>Next, we will activate the virtual environment</p>
<pre><code>source venv<span class="hljs-operator">/</span>bin<span class="hljs-operator">/</span>activate
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872429245/FGplErG0U.png" alt="image.png" />
The virtual environment can later be deactivated using </p>
<pre><code>deactivate
</code></pre><h3 id="heading-poetry">Poetry</h3>
<p>The first tool well be considering is poetry. Poetry is a dependency management tool for declaring, managing and installing dependencies for python projects. Poetry makes it to download the necessary dependencies and packages you may need for your project. It also makes it easy for you to package and publish yours. 
While we are not looking at building or packaging packages usually as data scientists, poetry provides a lot of other tools that may be beneficial to us. You can read more about poetry <a target="_blank" href="http://python-poetry.org/">here</a>.</p>
<p>To download poetry</p>
<pre><code><span class="hljs-attribute">curl</span> -sSL https://install.python-poetry.org | python3 -
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872450332/GdOYm8dYs.png" alt="image.png" />
You should have a result similar to that, after that add it to the path and check the version</p>
<pre><code>export PATH<span class="hljs-operator">=</span><span class="hljs-operator">/</span>home<span class="hljs-operator">/</span>yourusername<span class="hljs-operator">/</span>.local/bin<span class="hljs-operator">/</span>;$PATH
poetry version
</code></pre><p>You should also have a result similar to this.
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872464700/H8kIpdFA4.png" alt="image.png" /></p>
<p>It is also possible to use pip to install poetry</p>
<pre><code>pip <span class="hljs-keyword">install</span> <span class="hljs-comment">--user poetry</span>
</code></pre><p>That will produce a similar result to the method explained above.</p>
<p>To create a new project folder in poetry,</p>
<pre><code>poetry <span class="hljs-keyword">new</span> <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p>well check the folder structure of the new project using</p>
<pre><code>tree <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872487300/EjMVy_IYi.png" alt="image.png" />
Poetry creates a new-project folder for us inside the existing new-project folder. The structure of the project includes 2 directories:</p>
<ul>
<li><strong>new-project</strong>: This will house all our source codes except the test scripts.</li>
<li><strong>tests</strong>: This will house all the test scripts we will write for our project.</li>
</ul>
<p>2 additional files:</p>
<ul>
<li><strong>README.rst</strong>: This contains a readme description for your project.</li>
<li><strong>pyproject.toml</strong>: This will help you orchestrate your project and the dependencies youll be using. It will look something like this.</li>
</ul>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872509037/_qeyjtmc4.png" alt="image.png" /></p>
<h4 id="heading-remark">Remark</h4>
<p>Because poetry was not designed specifically with data scientists in mind, it means the folder will still need some adjustment to completely serve the need of data scientists as you will have to create new folders for your data, model outputs and others. In all, its a major improvement to the initial structure.</p>
<h3 id="heading-datasist">Datasist</h3>
<p>Datasist is actually a tool (library) built for data scientists for easy data analysis, visualisation, exploration and modelling. It was designed to extend the functionality of python data analysis tools like pandas and was built to provide a fast and quick abstraction interface for repetitive functions, codes and techniques in data science to one-line functions.</p>
<p>One important feature of datasist for creating project folder structure is the <strong>startproject</strong>. Because datasist was created specifically with data scientists in mind, thus the folder structure is more directed towards the needs of data scientists. Because our focus for this tutorial is setting up a project folder structure, our use of datasist will be limited to startproject only. You can read more about datasist <a target="_blank" href="https://github.com/risenW/datasist">here</a>.</p>
<p>To get started with datasist, well need to download datasist using the python package manager</p>
<pre><code><span class="hljs-attribute">pip</span> install datasist
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872530484/JHaKRMKb_.png" alt="image.png" />
It should start downloading and look something like mine. Once it is done, it should display </p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872546603/U6McWXCQh.png" alt="image.png" />
Next, we can ask datasist to create a new project folder for us using startproject</p>
<pre><code>startproject <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p>and lets check the project structure</p>
<pre><code>tree <span class="hljs-keyword">new</span><span class="hljs-operator">-</span>project
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872560896/wcea0witq.png" alt="image.png" /></p>
<p>And this produces a folder structure that is more designed for data scientists and data science projects.
A breakdown of the structure includes two files:</p>
<ul>
<li><strong>README.txt</strong>: This is a readme file containing the brief of our project and every information people need to know about it.</li>
<li><strong>config.txt</strong>: this is an automatic configuration file generated by datasist that contains the configuration of the whole project with the different path addresses.</li>
</ul>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872582090/ecXAp10cg.png" alt="image.png" /></p>
<p>The folder also contains 3 subfolders namely:</p>
<ul>
<li><strong>data</strong>: This will contain all the data files well be using for our project and it is further subdivided into:</li>
<li><ul>
<li><strong>raw</strong>: This folder will contain all the raw data files from our web scraping or data collation process. These are the data files that are still untouched and unprocessed.</li>
</ul>
</li>
<li><ul>
<li><strong>processed</strong>: This will folder contain our dataset after it has been processed and transformed. This prevents the mixing up of processed data with the raw data in the raw folder.</li>
</ul>
</li>
<li><p><strong>Output</strong>: This folder will contain all the outputs from our project (visualisations, models, parameters etc). This folder ensures that they are all kept safe and untampered with. It contains a subfolder:</p>
</li>
<li><ul>
<li><strong>models</strong>: This folder will host all the models for our project that we will need to save.</li>
</ul>
</li>
<li><p><strong>src</strong>: This will contain all the scripts and notebooks for our project. It is further divided into:</p>
</li>
<li><ul>
<li><strong>notebooks</strong>: This will contain all the notebooks for experimentation. Usually, because notebooks are kept out and not committed to git, we can easily specify for git to ignore all the contents of this folder.</li>
</ul>
</li>
<li><ul>
<li><strong>scripts</strong>: This will contain all the actual scripts well use for our project. It is further divided into 4 subfolders;</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><strong>ingest</strong>: This will contain the scripts for data ingestion and loading. Also, our web scraping scripts can be placed in this folder.</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><strong>modeling</strong>: This will contain all scripts for building our machine learning models</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><strong>preparation</strong>: This will contain the scripts for our data preparation and transformation.</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li><strong>test</strong>: This will contain all our tests for all the other scripts in our project.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="heading-remark">Remark</h4>
<p>Because datasist is a package designed for data scientists, the folder structure by <strong>startproject</strong> is designed to meet the basic needs of data scientists and get them running as soon as possible, thus the folder structure will serve the basic needs for any machine learning project.</p>
<h3 id="heading-cookiecutter">Cookiecutter</h3>
<p>The last tool well be looking at is cookiecutter. Cookiecutter is actually a package designed to create a project folder structure following an existing template. It is therefore designed for all developers to create folders structure to meet their specific needs using existing templates and in the absence of one, they can create a new template to meet their needs.</p>
<p>To get started, well need to install cookiecutter</p>
<pre><code><span class="hljs-attribute">pip</span> install cookiecutter
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872599892/4NzEmjmP0.png" alt="image.png" />
You should have a result similar to mine showing cookiecutter and its dependencies being downloaded.
Next, we need to find a cookiecutter template that suits our specific need, this refers to a project folder template designed to be used with cookiecutter for our specific need (you can search on GitHub for cookiecutter templates and well see numerous templates that exist that we can choose one from). For our case, well use the cookiecutter-data-science template that is available on GitHub (you can check it <a target="_blank" href="https://www.github.com/drivendata/cookiecutter-data-science">here</a>)</p>
<pre><code><span class="hljs-selector-tag">cookiecutter</span> <span class="hljs-selector-tag">https</span>:<span class="hljs-comment">//www.github.com/drivendata/cookiecutter-data-science</span>
</code></pre><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872620201/vR5kEtiX7.png" alt="image.png" />
It will ask us to enter the name of our project, the name of the repository, the authors name, a description of the project, an open-source licence, s3-bucket (if we have our data in the cloud which is optional), aws_profile (which is also optional) and python interpreter and after this will create a new folder structure for our project.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656872964512/KuYLpc5M4.png" alt="image.png" />
Based on the existing structure of the template of cookiecutter that we used, cookiecutter generated an exhaustive and very nice folder structure for us. This folder has 7 subfolders:</p>
<ul>
<li><strong>data</strong>: This folder will contain all the data we will use for the project. It is further subdivided into four; <strong>external</strong>, <strong>interim</strong>, <strong>processed</strong> and <strong>raw</strong> which are similar to that produced by datasist but with an external folder, for datasets that might be from external sources and interim for datasets that may contain compilation and combination of all the different data files from different data sources before we then process them for our use</li>
<li><strong>docs</strong>: This folder will contain all the necessary documentation for our project. This will include the <strong>Makefile</strong>, <strong>commands</strong> on how to download data to our local environment from our s3 bucket, <strong>configurations</strong> for sphinx to automatically generate documentation for our project, documentation on <strong>getting_started</strong>, the <strong>index</strong> file which will contain the master file (document) generated by sphinx and <strong>makefile.bat</strong> on instructions to print to the terminal whatever information we want there.</li>
<li><strong>model</strong>: This folder will contain all our trained and serialised models, model predictions and model summaries.</li>
<li><strong>notebooks</strong>: This will host all the notebooks we will use in our project during experimentation.</li>
<li><strong>references</strong>: This will contain all the references, manuals, data dictionaries and other explanatory materials used during our project.</li>
<li><strong>reports</strong>: This will contain all the latex, markdowns, pdfs, charts, visualisations and dashboards that we may need to produce during the course of our project. It also contains the <strong>figure</strong> subfolder to contain the visualisations.</li>
<li><strong>src</strong>: This will contain all the source codes for our projects. This is similar to the src folder of datasists folder structure but in addition to <strong>models</strong> and <strong>feature</strong> (instead of preprocessing), it also contains <strong>data</strong> as a replacement for ingest to make, create or scrap for datasets and <strong>visualisations</strong> for creating visualisations.</li>
</ul>
<p>The files created by the cookiecutter include;</p>
<ul>
<li><strong>LICENCE</strong>: which contains the licencing for our project based on the option we selected.</li>
<li><strong>Makefile</strong>: to execute commands like make data and make train.</li>
<li><strong>README.md</strong>: This will contain the description of our project that will be visible on the projects profile and other important information we want to pass to the users of our project.</li>
<li><strong>requirements.txt</strong>: This will contain a list of all dependencies and packages our project utilises alongside their versions to promote reproducibility.</li>
<li><strong>setup.py</strong>: to package our project to make it pip installable.</li>
<li>test_environmet.py: To test and ensure that the programming environment we are using / about to use is consistent with the one our project has been/<em>is being</em> written in.</li>
<li>tox.ini  a tox file with necessary settings for running tox; you can read more about tox <a target="_blank" href="http://tox.readthedocs.io/">here</a></li>
</ul>
<h4 id="heading-remark">Remark</h4>
<p>Cookiecutter when combined with the cookiecutter-data-science template seems like the best choice as it seamlessly creates a standard project folder for us with all the things we know we will need and others we arent even sure of yet. The only shortcoming of this folder is that it does not contain a test folder for us to keep all our test scripts, thus, we will need to do this ourselves which we can easily do using</p>
<pre><code><span class="hljs-keyword">mkdir</span> tests
</code></pre><h2 id="heading-conclusion">Conclusion</h2>
<p>While a lot of thought does not go into creating a folder structure for our projects, but when properly done, it can save us a huge amount of time and even help us be more productive.</p>
<p>I hope you really enjoyed this article. If you did, please share with someone who might also benefit from the information shared in this article, you can also like, drop positive feedback and subscribe so you dont miss out when I drop a new article. See you.</p>
</p>
      <p class="post-meta">
        1 min read &nbsp; &middot; &nbsp;
        July 1, 2022
        &nbsp; &middot; &nbsp; hashnode.com
      </p>
      <p class="post-tags">
        <a href="/blog/2022">
          <i class="fas fa-calendar fa-sm"></i> 2022 </a>

          

          
    </p></li>

    

    
    
    
    

    <li><h3>
        
          <a class="post-title" href="https://madeofajala.hashnode.dev/accuracy-recall-and-precision-understanding-when-to-use-which-and-business-importance" target="_blank">Accuracy, Recall and Precision: Understanding When To Use Which and Business importance</a>
          <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg">
            <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path>
          </svg>
        
      </h3>
      <p><h2 id="heading-introduction">Introduction</h2>
<p>Im sure every data scientist starts their data science journey by pumping those accuracy scores, trying to squeeze out an extra 0.5% to move their 99% accurate model to 99.5% while also trying to maintain a healthy balance of preventing overfitting, but all that mattered then, especially for those Kaggle, Zindi hackathons then was accuracy. 
But as we all progress up the data science ladder, we begin to learn about precision, recall, F1-score and how they are better means of evaluating our model performance. So how do you know when to use which (accuracy, precision, recall and f1-score)? In this article, Ill be taking you through that giving you a real industry perspective also. Lets get right into it.</p>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/OuePMznpbHyrw34608/giphy.gif">https://media.giphy.com/media/OuePMznpbHyrw34608/giphy.gif</a></div>
<h3 id="heading-getting-started">Getting Started</h3>
<p>So before we proceed to understand what actually is accuracy, precision, recall and f1-score are, lets, first of all, create a sample dataset and simple model that we will use to explain these concepts.</p>
<pre><code><span class="hljs-keyword">import</span> <span class="hljs-title">numpy</span> <span class="hljs-title"><span class="hljs-keyword">as</span></span> <span class="hljs-title">np</span>
<span class="hljs-title"><span class="hljs-keyword">from</span></span> <span class="hljs-title">sklearn</span>.<span class="hljs-title">datasets</span> <span class="hljs-title"><span class="hljs-keyword">import</span></span> <span class="hljs-title">make_classification</span>

<span class="hljs-title">X</span>, <span class="hljs-title">Y</span> <span class="hljs-operator">=</span> <span class="hljs-title">make_classification</span>(<span class="hljs-title">n_samples</span> <span class="hljs-operator">=</span> 500,
                          <span class="hljs-title">n_features</span> <span class="hljs-operator">=</span> 10,
                          <span class="hljs-title">n_informative</span> <span class="hljs-operator">=</span> 5,
                          <span class="hljs-title">n_redundant</span> <span class="hljs-operator">=</span> 5,
                          <span class="hljs-title">n_classes</span> <span class="hljs-operator">=</span> 2,
                          <span class="hljs-title">weights</span> <span class="hljs-operator">=</span> [0.75, 0.25],
                          <span class="hljs-title">random_state</span> <span class="hljs-operator">=</span> 27)
</code></pre><p>We are trying to create a simulated classification dataset that well be using with 500 rows, and 10 columns out of which only 5 are informative and 5 are noise. The data has 2 classes 0 and 1 with 400 zeros and 100 ones.
Next, we want to confirm that the dataset we created is just as expected and view the first 5 rows of X and the first 100 of Y</p>
<pre><code class="lang-python">X.shape
</code></pre>
<p>(500, 10)</p>
<pre><code><span class="hljs-selector-tag">Y</span><span class="hljs-selector-class">.shape</span>
</code></pre><p>(500,)</p>
<pre><code><span class="hljs-selector-tag">X</span><span class="hljs-selector-attr">[:5, :]</span>
</code></pre><p>array([[-0.32718827,  1.24378523, -1.63117202,  1.70196308,  3.22607374,
         0.12536251, -1.32531688, -1.5178169 ,  0.0391831 ,  5.37803965],
       [-0.18882151,  3.24450273, -0.04684192,  0.42667327,  0.95977818,
        -1.41401465,  0.6218516 , -2.76514217, -1.35078041,  1.62561421],
       [-1.32438294, -2.56364318, -3.10395577, -0.71218974,  2.06383318,
         2.2750899 , -0.41726115,  3.39746801,  0.48384514,  0.00984739],
       [ 2.72758704,  0.01696436,  0.8414895 , -0.42127927,  2.0270105 ,
         3.09904819, -0.80473595,  2.5118864 , -2.07646986, -0.56676343],
       [ 2.83704055, -1.64725719,  1.04781064,  1.69459622,  1.951591  ,
         1.24457155, -0.03491821,  2.14001566,  0.51324065,  0.93051073]])</p>
<pre><code><span class="hljs-selector-tag">Y</span><span class="hljs-selector-attr">[:100]</span>
</code></pre><p>array([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
           1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
           0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
           1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
           0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0])</p>
<p>Next, lets split our dataset to train and test set</p>
<pre><code><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> <span class="hljs-title">train_test_split</span>

<span class="hljs-title">X_train</span>, <span class="hljs-title">X_test</span>, <span class="hljs-title">Y_train</span>, <span class="hljs-title">Y_test</span> <span class="hljs-operator">=</span> <span class="hljs-title">train_test_split</span>(
                                <span class="hljs-title">X</span>,<span class="hljs-title">Y</span>, <span class="hljs-title">test_size</span><span class="hljs-operator">=</span>0.2, <span class="hljs-title">random_state</span><span class="hljs-operator">=</span>27)
</code></pre><p>We split our dataset to keep 20% (100 rows) as test_size and use others to train.
Next, lets ascertain the size of our splits</p>
<pre><code><span class="hljs-selector-tag">X_train</span><span class="hljs-selector-class">.shape</span>
</code></pre><p>(400, 10)</p>
<pre><code><span class="hljs-selector-tag">Y_train</span><span class="hljs-selector-class">.shape</span>
</code></pre><p>(400,)</p>
<pre><code><span class="hljs-selector-tag">X_test</span><span class="hljs-selector-class">.shape</span>
</code></pre><p>(100, 10)</p>
<pre><code><span class="hljs-selector-tag">Y_test</span><span class="hljs-selector-class">.shape</span>
</code></pre><p>(100,)</p>
<p>Next, lets create a simple Logistic Regression model to train our dataset.</p>
<pre><code><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> <span class="hljs-title">LogisticRegression</span>

<span class="hljs-title">logit</span> <span class="hljs-operator">=</span> <span class="hljs-title">LogisticRegression</span>(<span class="hljs-title">random_state</span><span class="hljs-operator">=</span>27)
<span class="hljs-title">model</span> <span class="hljs-operator">=</span> <span class="hljs-title">logit</span>.<span class="hljs-title">fit</span>(<span class="hljs-title">X_train</span>, <span class="hljs-title">Y_train</span>)
<span class="hljs-title">Y_hat</span> <span class="hljs-operator">=</span> <span class="hljs-title">model</span>.<span class="hljs-title">predict</span>(<span class="hljs-title">X_test</span>)
<span class="hljs-title">Y_proba</span> <span class="hljs-operator">=</span> <span class="hljs-title">model</span>.<span class="hljs-title">predict_proba</span>(<span class="hljs-title">X_test</span>) 
<span class="hljs-title">Y_proba</span>.<span class="hljs-title">shape</span>
</code></pre><p>(100,2)</p>
<p>Next, well be importing the metrics well be using to evaluate our model</p>
<pre><code><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score

print(<span class="hljs-string">f"Training accuracy: <span class="hljs-subst">{model.score(X_train, Y_train)}</span>"</span>)
print(<span class="hljs-string">f"Test accuracy: <span class="hljs-subst">{model.score(X_test, Y_test)}</span>"</span>)
</code></pre><p>Training accuracy: 0.8175</p>
<p>Test accuracy: 0.75</p>
<p>Also before we dive into the theory behind the topics, Ill write a helper function that will help us tinker with our prediction, Ill explain briefly what it does after</p>
<pre><code>def set_threshold(Y_proba, threshold):
    Y_hat <span class="hljs-operator">=</span> []
    <span class="hljs-keyword">for</span> x in Y_proba[:, <span class="hljs-number">1</span>]:
        <span class="hljs-keyword">if</span> x <span class="hljs-operator">&gt;</span> threshold: Y_hat.append(<span class="hljs-number">1</span>)
        <span class="hljs-keyword">else</span>: Y_hat.append(<span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> Y_hat
</code></pre><p>This helper function basically helps us set the boundary for our decisions. Our logistic regression gives a probability of our inputs being the 0 or 1, there are two columns when we use model.predict_proba (which well be using for this analysis) instead of model.predict. Logistic regression thus uses a default threshold of 0.5, which means if for the predictions Y_proba[:,0] (all rows of probability predictions of 0), if the model is more than 50% sure (i.e value is greater than 0.5), then return 0 for that row, else it is 1. The same thing for the second column representing 1 (dont forget, the sum of both rows equals 1), so well be selecting just the second row, if the value is greater than our threshold (in case we want greater confidence, lets say 80% in our predictions or lower confidence), it should return 1, if not, its automatically a 0.</p>
<h3 id="heading-under-the-hood">Under the hood</h3>
<p>So what exactly is accuracy, precision or recall? To understand that, well need to first understand what a confusion matrix is.</p>
<h4 id="heading-confusion-matrix">Confusion matrix</h4>
<p>So a confusion matrix is a two-by-two matrix where the rows represent the actual values while the columns represent the predicted values</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656377591943/KGLbAqsOm.png" alt="Screenshot from 2022-06-27 13-08-39.png" /></p>
<p>For the sake of this analysis, 1 will be our positive value while 0 will be the negative value. The four constituents of a confusion matrix as shown above are TN, FN, FP and TP</p>
<ul>
<li><strong>True Negatives (TN)</strong>: This represents the actual negative values (0 in our case) that the model predicted as negative.</li>
<li><strong>False Negative (FN)</strong>: This represents values that are actually positive but or model predicted as negative (actually 1 but predicted as 0).</li>
<li><strong>False Positives (FP)</strong>: This represents values that are actually negative but our model predicted as positive (actually 0 but predicted as 1).</li>
<li><strong>True Positive (TP)</strong>: This represents values that are true and our model also predicted as true (1 predicted as 1 also).</li>
</ul>
<p>This means the first row represents all values that are actually negative (0), while the second row represents values that are actually positive (1). The first column thus also represents all the values our model predicted as negative (0) while the second column represents the values our model predicted as positive (1).  Before we proceed any further, let us create the confusion matrix for the initial prediction.</p>
<pre><code><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> <span class="hljs-title">confusion_matrix</span>

<span class="hljs-title">confusion_matrix</span>(<span class="hljs-title">Y_test</span>, <span class="hljs-title">Y_hat</span>)
</code></pre><pre><code><span class="hljs-string">array([[65,</span>  <span class="hljs-number">3</span><span class="hljs-string">],</span>
       [<span class="hljs-number">22</span>, <span class="hljs-number">10</span>]<span class="hljs-string">])</span>
</code></pre><blockquote>
<p>[Note that the values inside confusion_matrix function are positional, it expects the base truth (Y_test) as the first argument and the predictions (Y_hat) as the second, anything apart from this can mess up your analysis].</p>
</blockquote>
<p>Thus, we can assign the values as follows TN (65), FN (22), FP (3) and TP (10). Now that we understand what a confusion matrix is, we can move on to others.</p>
<h4 id="heading-accuracy">Accuracy</h4>
<p>Accuracy is therefore the sum of actual correct predictions (TP and TN) by our model, divided by total test data size, simply,</p>
<pre><code> Accuracy <span class="hljs-operator">=</span> (TN <span class="hljs-operator">+</span> TP)<span class="hljs-operator">/</span>(TN<span class="hljs-operator">+</span> FN <span class="hljs-operator">+</span> FP <span class="hljs-operator">+</span> TP).
</code></pre><p>This gives a simplistic overview of how correct our model is, nothing more than that.
This means the accuracy of our model is</p>
<pre><code>Accuracy <span class="hljs-operator">=</span> (<span class="hljs-number">65</span> <span class="hljs-operator">+</span><span class="hljs-number">10</span>)<span class="hljs-operator">/</span>(<span class="hljs-number">65</span> <span class="hljs-operator">+</span> <span class="hljs-number">22</span> <span class="hljs-operator">+</span> <span class="hljs-number">3</span> <span class="hljs-operator">+</span> <span class="hljs-number">10</span>)
<span class="hljs-operator">=</span>   <span class="hljs-number">75</span><span class="hljs-operator">/</span><span class="hljs-number">100</span>
<span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-number">.75</span>
</code></pre><h4 id="heading-precision">Precision</h4>
<p>Precision however tells us of how many of the values our model predicted as positive are actually positive.</p>
<pre><code>Precision <span class="hljs-operator">=</span> TP<span class="hljs-operator">/</span>(TP <span class="hljs-operator">+</span> FP)
</code></pre><p>This means the precision for our model is </p>
<pre><code>Precision <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-operator">/</span>(<span class="hljs-number">10</span><span class="hljs-operator">+</span><span class="hljs-number">3</span>)
    <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-operator">/</span><span class="hljs-number">13</span>
    <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-number">.7692</span>
</code></pre><p>Precision is used when there is a great cost for FPs, Ill explain this very soon</p>
<h4 id="heading-recall">Recall</h4>
<p>Recall tells us how many of the actual positive value is captured as positive by our model. This calculated using</p>
<pre><code>Recall <span class="hljs-operator">=</span> TP<span class="hljs-operator">/</span>(FN <span class="hljs-operator">+</span> TP)
</code></pre><p>This means the recall for our model is </p>
<pre><code>Recall <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-operator">/</span>(<span class="hljs-number">22</span><span class="hljs-operator">+</span><span class="hljs-number">10</span>)
            <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-operator">/</span><span class="hljs-number">32</span>
            <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-number">.3125</span>
</code></pre><p>Recall on the other hand is used when there is a great cost for every FN. This Ill also explain soon.
These values can be gotten simply in sklearn using the classification_report method from metrics.</p>
<pre><code><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> <span class="hljs-title">classification_report</span>

<span class="hljs-title">print</span>(<span class="hljs-title">classification_report</span>(<span class="hljs-title">Y_test</span>, <span class="hljs-title">Y_hat</span>))
</code></pre><pre><code>                   <span class="hljs-attribute">precision</span>    recall  f<span class="hljs-number">1</span>-score   support

                    <span class="hljs-attribute">0</span>      <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">96</span>      <span class="hljs-number">0</span>.<span class="hljs-number">84</span>        <span class="hljs-number">68</span>
                    <span class="hljs-attribute">1</span>       <span class="hljs-number">0</span>.<span class="hljs-number">77</span>      <span class="hljs-number">0</span>.<span class="hljs-number">31</span>      <span class="hljs-number">0</span>.<span class="hljs-number">44</span>        <span class="hljs-number">32</span>

     <span class="hljs-attribute">accuracy</span>                                 <span class="hljs-number">0</span>.<span class="hljs-number">75</span>       <span class="hljs-number">100</span>
     <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">76</span>      <span class="hljs-number">0</span>.<span class="hljs-number">63</span>      <span class="hljs-number">0</span>.<span class="hljs-number">64</span>       <span class="hljs-number">100</span>
 <span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">71</span>       <span class="hljs-number">100</span>
</code></pre><blockquote>
<p>[Note that the values inside classification_report function are positional, it expects the base truth (Y_test) as the first argument and the predictions (Y_hat) as the second, anything apart from this can mess up your analysis]</p>
</blockquote>
<p>This gives the precision, recall, f1 score (to be explained also) of each class and weighted accuracy.</p>
<h3 id="heading-when-to-use-which">When to use which</h3>
<p>So if youre observant, youll notice two things, first, the precision of 0 is relatively good (0.75) and the recall is even much higher (0.96) even though the accuracy is 0.75. This happened because our dataset is unbalanced (this means the two classes are not of the same number or weight, we have 68 values that are actually zeros and 32 values that are ones). The implication of this is that if we set our model to return the highest class (0) for every prediction, our model will have an accuracy of 0.68 (since we will get all 0s and miss all 1s). This point to the fact that accuracy is not a reliable metric to evaluate a model in cases of unbalanced dataset (e.g cancer detection, where out of 100,000 thousand people screened just 10 people might actually have cancer).</p>
<p>The other thing to note is that the recall and precision of 1 are like inversely proportional. This is because if we want high precision, we need our model to be very confident that the prediction it is making is actually positive, thus any value that is actually positive but our model is not so confident about i.e didnt meet our threshold will be set as positive. This means the actually positive values will be the majority of values that our model will predict as positive. This also then implies that, the cost of this high precision will a lot of actually positive values but which our model is not confident about will be classified as negative, therefore we will have a lot of false negatives. This implies that the values our model will predict as positive will actually be a very small fraction of actually positive values, thus low recall. The inverse will give a high recall and low precision.</p>
<p>Lets's go through this using business case studies.</p>
<h3 id="heading-business-case-studies">Business case studies</h3>
<p>Here Ill be showing two different case studies;</p>
<ul>
<li>Fraud detection</li>
<li>Drug discovery </li>
</ul>
<p>using different thresholds so that we can have a very high recall for one and very high precision for the other.
A little introduction to our case studies to understand their business needs.</p>
<h4 id="heading-fraud-detection">Fraud detection</h4>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/hgjNPEmAmpCMM/giphy.gif">https://media.giphy.com/media/hgjNPEmAmpCMM/giphy.gif</a></div>
<p>X is a new fintech startup that is trying to compete in the neobank space with Kuda, Carbon, Renmoney et al in Nigeria and Africa. But there is a problem, fraud is rampant in the space. So they are employing you as a data scientist to build a machine learning model to detect whether a transaction is fraudulent or not.
As the data scientist, you have built a fairly accurate model using Logistic Regression but you understand that accuracy wont tell you the full picture, so you need something to give you more insight about your model and whether to increase your threshold for what you classify as fraud or not, so lets break this down using the confusion matrix for what the result for each prediction will be on the business.</p>
<ul>
<li><strong>True Positive</strong>: These are transactions that are actually frauds and your model also predicted as frauds. You want to increase this to capture all fraudulent transactions as actual frauds.</li>
<li><strong>True Negative</strong>: These are transactions that are not frauds (i.e normal transactions between your customers) and your model predicted correctly too.</li>
<li><strong>False Positive</strong>: These are normal transactions between your users but your model flagged as fraud. By flagging them, the users will have to contact your boss that they are the ones performing the transactions and are not committing any fraud. This may cause your customers inconvenience and make your companys service offering not as sleek as they need it to be. If it consistently flags people from a particular region or demography, your company may receive public out-lash that it is biased and people from that region may boycott your product and others who want to stand in solidarity. You want to reduce this to as low as possible.</li>
<li><strong>False Negatives</strong>: These are transactions that are actually frauds but your model predicted that they are not. As a data scientist in this company, you dont even want to see this at all because it threatens both your company and your job. If you model often flag transactions that are frauds as not fraud, your company will have to pay that money back to the client or pay several damages if many people sue, so the company may use all the runway money to pay damages and you people will pack up or if not, your CEO may think maybe youre conniving with the fraudsters to give them easy passage and sue you or even think youre probably not good enough in your job and end up sacking you to employ another person. (These are just hypothetical scenarios).</li>
</ul>
<p>So you understand that while you want to reduce both the FP and FNs, your customers may still understand FP scenarios a bit, they wont tolerate FN at all. This means there is a greater cost associated with FN than FP. Thus, the goal is to have the best recall as high as possible while maintaining relatively good precision and accuracy</p>
<h4 id="heading-drug-discovery">Drug discovery.</h4>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/2vr6AHqSMvEIPipLcd/giphy.gif">https://media.giphy.com/media/2vr6AHqSMvEIPipLcd/giphy.gif</a></div>
<p>Y is a new tech-enabled pharmaceutical company that is trying to bring a new drug to market. Drug discovery is a very expensive and time-consuming process with a new drug taking an average of about 10 years before getting to the market and requiring an estimated $10 billion. Because your company is tech-focused, they employed you as a data scientist to build a machine learning model on top of the High Virtual Through Put Screening (HTVS) to determine whether a new compound will be a potential drug but there is a potential cost of $100k to evaluate and carry out necessary tests to check whether the compound will actually be useful.
So as a data scientist, you have also built a simple model using logistic regression and want to evaluate the model better beyond just accuracy to using confusion matrix, precision and recall. So lets break down using the confusion matrix what  each prediction means and the business impact:</p>
<ul>
<li><strong>True Positive</strong>: These are actual compounds that have a therapeutic activity that your model predicted as positive compounds, you want to optimise this as much as possible.</li>
<li><strong>True Negative</strong>: These are compounds that actually have no therapeutic activity and your model predicted correctly as negative compounds not to take note of. You also want to maintain this.</li>
<li><strong>False Negative</strong>: These are compounds that actually have therapeutic values but our model predicted as negative (that they have no therapeutic value). You want to reduce this as much as possible because those are potential compounds that may be the solution to some of the biggest health challenges globally and may have the highest ROI (return on investment).</li>
<li><strong>False Positive</strong>: These are compounds that dont actually have any therapeutic value or are even toxic but our model predicted them as positive (i.e they have therapeutic importance). You dont even want to see these at all because, for every one of them, that is $100k going down the drain, this means 10 FPs is $ 1 million all for nothing, thats a lot of money your company is looking for you to save for them.
(These are just hypothetical scenarios).</li>
</ul>
<p>So based on this, while we want to reduce both FP and FN, we can see that there is a greater cost to FP compared to FN, thus, our priority is having the best precision possible while maintaining relatively good recall and the accuracy too.</p>
<h3 id="heading-original-model">Original model</h3>
<p>So heres a copy of the result of our initial model (with the threshold set to 0.5), its confusion matrix, accuracy, precision and recall.</p>
<pre><code><span class="hljs-attribute">Y_50</span> = set_threshold(Y_proba, threshold=<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)
<span class="hljs-attribute">print</span>(<span class="hljs-string">"Confusion matrix with thresh = 0.5 \n"</span>,
      <span class="hljs-attribute">confusion_matrix</span>(Y_test, Y_hat))
</code></pre><pre><code><span class="hljs-string">Confusion</span> <span class="hljs-string">matrix</span> <span class="hljs-string">with</span> <span class="hljs-string">thresh</span> <span class="hljs-string">=</span> <span class="hljs-number">0.5</span> 
 [[<span class="hljs-number">65</span>  <span class="hljs-number">3</span>]
 [<span class="hljs-number">22</span> <span class="hljs-number">10</span>]]
</code></pre><pre><code><span class="hljs-title">print</span>(classification_re<span class="hljs-keyword">port</span>(Y_test,Y_50))
</code></pre><pre><code>                       <span class="hljs-attribute">precision</span>    recall  f<span class="hljs-number">1</span>-score   support

                   <span class="hljs-attribute">0</span>      <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">96</span>      <span class="hljs-number">0</span>.<span class="hljs-number">84</span>        <span class="hljs-number">68</span>
                   <span class="hljs-attribute">1</span>       <span class="hljs-number">0</span>.<span class="hljs-number">77</span>      <span class="hljs-number">0</span>.<span class="hljs-number">31</span>      <span class="hljs-number">0</span>.<span class="hljs-number">44</span>        <span class="hljs-number">32</span>

   <span class="hljs-attribute">accuracy</span>                                  <span class="hljs-number">0</span>.<span class="hljs-number">75</span>       <span class="hljs-number">100</span>
    <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">76</span>      <span class="hljs-number">0</span>.<span class="hljs-number">63</span>      <span class="hljs-number">0</span>.<span class="hljs-number">64</span>       <span class="hljs-number">100</span>
<span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">71</span>       <span class="hljs-number">100</span>
</code></pre><ul>
<li><p><strong>Fraud detection</strong>: For our fraud detection scenario, the model has an accuracy of 0.75, a precision of 0.77 and a recall of 0.31. The accuracy and precision are fairly good but the recall is bad. Remember, recall means the number of the actual positive value that our model captured as true and we need to optimise our fraud detection model to have the best recall (low FNs). The implication of that recall means that our model is only flagging 10 out of 32 fraudulent transactions as fraudulent while fraudsters are having a swell time with the remaining 22 transactions. This implies that with our 77% precision, this is a really bad model.</p>
</li>
<li><p><strong>Drug discovery</strong>: For our drug discovery model, a precision of 77% means out of 13 compounds that our model predicted as potential drugs, 10 are actual potential drugs while just 3 are wrong, while there is a huge cost on the 3 errors this is still a relatively good model, the cost is not that much and given that we were able to identify 10 potential drugs out 100 that we were given. The poor recall shows that we are missing out on a lot of potential compounds that may be potential drugs, but we are at least not wasting money chasing compounds that actually are not. In summary, this is a relatively good model for our drug discovery.</p>
</li>
</ul>
<h3 id="heading-lower-threshold">Lower threshold</h3>
<p>Next, well set a lower threshold/ confidence limit of 0.3 for our model, this means probability predictions of 1s greater than 0.3 will be classified as 1 or otherwise as 0.</p>
<pre><code><span class="hljs-attribute">Y_30</span> = set_threshold(Y_proba, threshold=<span class="hljs-number">0</span>.<span class="hljs-number">3</span>)
<span class="hljs-attribute">print</span>(<span class="hljs-string">"Confusion matrix with thresh = 0.3 \n"</span>,
      <span class="hljs-attribute">confusion_matrix</span>(Y_test, Y_<span class="hljs-number">30</span>,))
</code></pre><pre><code><span class="hljs-string">Confusion</span> <span class="hljs-string">matrix</span> <span class="hljs-string">with</span> <span class="hljs-string">thresh</span> <span class="hljs-string">=</span> <span class="hljs-number">0.3</span> 
 [[<span class="hljs-number">59</span>  <span class="hljs-number">9</span>]
 [ <span class="hljs-number">8</span> <span class="hljs-number">24</span>]]
</code></pre><pre><code><span class="hljs-title">print</span>(classification_re<span class="hljs-keyword">port</span>(Y_test, Y_30))
</code></pre><pre><code>                        <span class="hljs-attribute">precision</span>    recall  f<span class="hljs-number">1</span>-score   support

                    <span class="hljs-attribute">0</span>       <span class="hljs-number">0</span>.<span class="hljs-number">88</span>      <span class="hljs-number">0</span>.<span class="hljs-number">87</span>      <span class="hljs-number">0</span>.<span class="hljs-number">87</span>        <span class="hljs-number">68</span>
                     <span class="hljs-attribute">1</span>       <span class="hljs-number">0</span>.<span class="hljs-number">73</span>      <span class="hljs-number">0</span>.<span class="hljs-number">75</span>      <span class="hljs-number">0</span>.<span class="hljs-number">74</span>        <span class="hljs-number">32</span>

        <span class="hljs-attribute">accuracy</span>                                 <span class="hljs-number">0</span>.<span class="hljs-number">83</span>       <span class="hljs-number">100</span>
      <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">81</span>      <span class="hljs-number">0</span>.<span class="hljs-number">81</span>       <span class="hljs-number">100</span>
<span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">83</span>      <span class="hljs-number">0</span>.<span class="hljs-number">83</span>      <span class="hljs-number">0</span>.<span class="hljs-number">83</span>       <span class="hljs-number">100</span>
</code></pre><ul>
<li><p><strong>Fraud detection</strong>: This model has much higher accuracy and recall compared to the last model but slightly lower precision. Analysis of this model shows that it accurately predicts 24 out of 32 fraudulent transactions as actually fraudulent. This is a lot better model than the previous model. Even though the false negatives are a bit still high (8), it is a lot better than the previous (22). The slightly lower precision and confusion matrix showed we have more false positives i.e more customers whose normal transactions we are flagged as fraudulent and have to be put through the stress of proving themselves compared to the previous model (with 3) but in all, this is a lot better model for us.</p>
</li>
<li><p><strong>Drug discovery</strong>: For our drug discovery model, the higher recall means this is a great model, we have been able to identify more potential drugs (24) compared to the former model (10). That is definitely great business. The slight drop in precision means compared to 3 false positives that the previous model gave, this model gave 9, the financial implication of that is the company is wasting an additional $600K more than the $300k lost to false predictions of the previous model totally $900K. Thats definitely not so great business. This means that even though this improved model gave us more compounds, our losses tripled. Depending on who you ask and how you look at it, it seems the previous model is better than this.</p>
</li>
</ul>
<h3 id="heading-higher-threshold">Higher threshold</h3>
<p>For this final model, we want to set the threshold/ confidence limit of our model to be very high (0.8).</p>
<pre><code><span class="hljs-attribute">Y_80</span> = set_threshold(Y_proba, threshold=<span class="hljs-number">0</span>.<span class="hljs-number">8</span>)
<span class="hljs-attribute">print</span>(<span class="hljs-string">"Confusion matrix with thresh = 0.8 \n"</span>,
      <span class="hljs-attribute">confusion_matrix</span>(Y_test, Y_<span class="hljs-number">80</span>,))
</code></pre><pre><code><span class="hljs-string">Confusion</span> <span class="hljs-string">matrix</span> <span class="hljs-string">with</span> <span class="hljs-string">thresh</span> <span class="hljs-string">=</span> <span class="hljs-number">0.8</span> 
 [[<span class="hljs-number">68</span>  <span class="hljs-number">0</span>]
 [<span class="hljs-number">29</span>  <span class="hljs-number">3</span>]]
</code></pre><pre><code><span class="hljs-title">print</span>(classification_re<span class="hljs-keyword">port</span>(Y_test, Y_80))
</code></pre><pre><code>                         <span class="hljs-attribute">precision</span>    recall  f<span class="hljs-number">1</span>-score   support

                     <span class="hljs-attribute">0</span>      <span class="hljs-number">0</span>.<span class="hljs-number">70</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">82</span>        <span class="hljs-number">68</span>
                     <span class="hljs-attribute">1</span>       <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">09</span>      <span class="hljs-number">0</span>.<span class="hljs-number">17</span>        <span class="hljs-number">32</span>

         <span class="hljs-attribute">accuracy</span>                                 <span class="hljs-number">0</span>.<span class="hljs-number">71</span>       <span class="hljs-number">100</span>
     <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">85</span>      <span class="hljs-number">0</span>.<span class="hljs-number">55</span>      <span class="hljs-number">0</span>.<span class="hljs-number">50</span>       <span class="hljs-number">100</span>
 <span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">71</span>      <span class="hljs-number">0</span>.<span class="hljs-number">62</span>       <span class="hljs-number">100</span>
</code></pre><ul>
<li><p><strong>Fraud detection</strong>: While the accuracy of this model (0.71) is not so far from the first model (0.75), it seems this is the worst model in the fraud detection case study, even though the precision is very high at 1.00 meaning we falsely classified no transaction as fraud that wasnt actually a fraud (all transactions classified as fraudulent are fraudulent), the cost is that we only classified 3 values as fraud while falsely classifying 29 transactions that are actually fraudulent as not fraudulent. This is a very bad model, a very bad one.</p>
</li>
<li><p><strong>Drug discovery</strong>: This precision of 1 of this model means that out of 3 values that we predicted as potential drugs, all the 3 are potential drugs. This is a really great model because we are not going to be wasting any money on any compound only to discover that is not a potential drug. For a small company with limited funds, this is a great model even though theyve missed out on a lot of other potential drugs. Depending on the size of the company and if they have any money to spare, if they are a small company with no money to spare, this is the best model for them. If they have some cash that they can burn, then the first model might be the best for them or if they are a large established company with all the money to burn just to get the best ROI, then the second model is for them.</p>
</li>
</ul>
<h3 id="heading-f1-score">F1 score</h3>
<p>So I have explained precision and recall and how they are both inversely proportional. What about times when we want to keep an eye on both our precision and recall using a single value?
<strong>F1 Score</strong> provides us with a value to keep an eye on both precision and recall simultaneously, it represents the harmonic mean of precision and recall and is given by</p>
<pre><code>F1 score <span class="hljs-operator">=</span> <span class="hljs-number">2</span> <span class="hljs-operator">*</span> (precision <span class="hljs-operator">*</span> recall)<span class="hljs-operator">/</span> (precision <span class="hljs-operator">+</span> recall)
</code></pre><p>The higher the F1 score, the relatively high values for both the precision and recall, if not, then one of the two values is really low. This shows in our dataset as the third model has an F1 score of 0.17 as the recall was 0.09 even though the accuracy was 0.71. The first model also had an F1 score of  0.44 because the recall was 0.31 even though the accuracy was 0.75, the second model which had relatively high precision and recall also had a high f1 score of 0.74 and even higher accuracy of 0.83.</p>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/DAtJCG1t3im1G/giphy.gif">https://media.giphy.com/media/DAtJCG1t3im1G/giphy.gif</a></div>
<p> 
In this article, I explained comprehensively precision, recall, accuracy and f1 score while considering business case studies and the considerations that go on behind the scene in selecting the best evaluation metric. If you found this article useful, please share it with someone who might also benefit from it. You can also share positive feedback in the comments or follow me on Twitter <a target="_blank" href="https://www.twitter.com/madeofajala">@madeofajala</a>. Till next time, see ya...</p>
</p>
      <p class="post-meta">
        1 min read &nbsp; &middot; &nbsp;
        June 24, 2022
        &nbsp; &middot; &nbsp; hashnode.com
      </p>
      <p class="post-tags">
        <a href="/blog/2022">
          <i class="fas fa-calendar fa-sm"></i> 2022 </a>

          

          
    </p></li>

    

    
    
    
    

    <li><h3>
        
          <a class="post-title" href="https://madeofajala.hashnode.dev/building-a-memory-efficient-etl-to-convert-json-to-csv-in-python-with-prefect" target="_blank">Building a memory-efficient ETL to convert JSON to CSV in Python with prefect</a>
          <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg">
            <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path>
          </svg>
        
      </h3>
      <p><p>Have you ever built a simple ETL (Extract-Transform-Load) pipeline in python to convert a JSON file to CSV? Easy peasy, right????? But have you ever tried to transform a large dataset (Large here meaning a dataset in which the dataset size is larger than your PC's memory) That should be interesting, isnt it? In this tutorial, I will be showing you how to do this in python. Now let's get into it.</p>
<div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://giphy.com/gifs/storyful-nba-drake-speech-7WvAUvZZTRpSuudobh">https://giphy.com/gifs/storyful-nba-drake-speech-7WvAUvZZTRpSuudobh</a></div>
<h2 id="heading-tools-well-be-using">Tools well be using:</h2>
<p>JSON, CSV, prefect, logging, tqdm</p>
<h2 id="heading-dataset">Dataset:</h2>
<p>The dataset well be using is the Amazon clothing and fashion dataset which is about  5.1GB in size, you can download it <a target="_blank" href="https://nijianmo.github.io/amazon/index.html">here</a>
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1656186784802/cPRuGxJu1.png" alt="image.png" /></p>
<h2 id="heading-why-should-this-interest-you">Why should this interest you?</h2>
<p>Most data scientist when starting their career starts with ready-made CSV files that they load directly into pandas with pandas.read_csv(), but the reality is in practice, most companies dont have their dataset readily available, which means most times, the data scientist needs to source their data, from databases, the internet and anywhere the data may be. This is where JSON comes in, JSON is the data format that rules the web, data sent and received on the web, between servers are usually in JSON format. This means it is important to know how to work with JSON files as a data scientist.</p>
<p><strong>So what is ETL?</strong> ETL standing for Extract-Transform-Load is a data engineer term used to describe the process by which data is extracted (from the web or database), transformed to add or remove data points that may not be in the original data point and load it in a format the data scientist can easily work with it, usually in CSV format. So basically an ETL pipeline is a data pipeline by which data is extracted, transformed and loaded in a ready-to-use format.
Pipeline means a sequence of functions tied together to perform a particular task.
So lets dive straight into it.</p>
<h2 id="heading-challenge">Challenge?</h2>
<p>The two main challenges we have are: 
     The size of the dataset (which is about 5.1GB)
     The RAM size of the pc we are using (which is 4GB)
This poses a challenge meaning we cant load the entire dataset to memory because well run out of memory. This means we have to look for a smart way to work with the dataset without running out of memory or crashing our PC.
The other problem is data schema is not so consistent, this means while the basic keys in each line are consistent, the style key contains another dictionary that is which are not the same for all rows in the dataset, i.e depending on the product, the style key contains descriptions of the product. </p>
<pre><code>  {{<span class="hljs-string">'overall'</span>: <span class="hljs-number">5.0</span>, <span class="hljs-string">'vote'</span>: <span class="hljs-string">'2'</span>, <span class="hljs-string">'verified'</span>: True, <span class="hljs-string">'reviewTime'</span>: <span class="hljs-string">'05 4, 2014'</span>, <span class="hljs-string">'reviewerID'</span>:      <span class="hljs-string">'A2IC3NZN488KWK'</span>,   <span class="hljs-string">'asin'</span>: <span class="hljs-string">'0871167042'</span>, <span class="hljs-operator">*</span><span class="hljs-operator">*</span><span class="hljs-string">'style'</span>: {<span class="hljs-string">'Format:'</span>: <span class="hljs-string">' Paperback'</span>},<span class="hljs-operator">*</span><span class="hljs-operator">*</span> <span class="hljs-string">'reviewerName'</span>: <span class="hljs-string">'Ruby    Tulip'</span>, <span class="hljs-string">'reviewText'</span>: <span class="hljs-string">'This book has beautiful photos, good and understandable directions, and many    different kinds of jewelry.  Wire working and metalsmithing jewelry are covered.  Highly recommend this book.'</span>, <span class="hljs-string">'summary'</span>: <span class="hljs-string">'Unique designs'</span>, <span class="hljs-string">'unixReviewTime'</span>: <span class="hljs-number">1399161600</span>}
{<span class="hljs-string">'overall'</span>: <span class="hljs-number">5.0</span>, <span class="hljs-string">'verified'</span>: True, <span class="hljs-string">'reviewTime'</span>: <span class="hljs-string">'08 12, 2017'</span>, <span class="hljs-string">'reviewerID'</span>: <span class="hljs-string">'A3HWZ3ARDF51IL'</span>, <span class="hljs-string">'asin'</span>: <span class="hljs-string">'B00009ZM7Z'</span>, <span class="hljs-operator">*</span><span class="hljs-operator">*</span><span class="hljs-string">'style'</span>: {<span class="hljs-string">'Size:'</span>: <span class="hljs-string">' 10.5 D(M) US'</span>, <span class="hljs-string">'Color:'</span>: <span class="hljs-string">' Fudge'</span>},<span class="hljs-operator">*</span><span class="hljs-operator">*</span> <span class="hljs-string">'reviewerName'</span>: <span class="hljs-string">'Amazon Customer'</span>, <span class="hljs-string">'reviewText'</span>: <span class="hljs-string">"My husband loves his Merrells. He won't wear any other brand at this point."</span>, <span class="hljs-string">'summary'</span>: <span class="hljs-string">'Best brand - so comfy'</span>, <span class="hljs-string">'unixReviewTime'</span>: <span class="hljs-number">1502496000</span>}
{<span class="hljs-string">'reviewerID'</span>: <span class="hljs-string">'A3VJX8VUVEKG3X'</span>, <span class="hljs-string">'asin'</span>: <span class="hljs-string">'B0001XVUFA'</span>, <span class="hljs-string">'reviewerName'</span>: <span class="hljs-string">'Parisa Diba'</span>, <span class="hljs-string">'verified'</span>: True, <span class="hljs-string">'reviewText'</span>: <span class="hljs-string">'Awesome'</span>, <span class="hljs-string">'overall'</span>: <span class="hljs-number">5.0</span>, <span class="hljs-string">'reviewTime'</span>: <span class="hljs-string">'06 15, 2016'</span>, <span class="hljs-string">'summary'</span>: <span class="hljs-string">'Five Stars'</span>, <span class="hljs-string">'unixReviewTime'</span>: <span class="hljs-number">1465948800</span>}}
</code></pre><p>So we need to find a way to extract all the columns in the dataset and represent them at the header of the dataset.</p>
<h2 id="heading-how-to-go-about-it">How to go about it</h2>
<p>Lets import the necessary libraries well be using.</p>
<pre><code># <span class="hljs-type">Loading</span> necessary libraries
<span class="hljs-keyword">import</span> json
from tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> csv
from prefect <span class="hljs-keyword">import</span> task, Flow
<span class="hljs-keyword">import</span> logging
</code></pre><p>A breakdown of what well be using each of the libraries for 
     json: to read the JSON file
     tqdm: to view updates or progress of our function running
     csv: to write in CSV format to the file
     prefect: to set up our ETL
Next, let us set us the log configuration for the project</p>
<pre><code>logging.basicConfig(
    format<span class="hljs-operator">=</span><span class="hljs-string">"%(asctime)s [%(levelname)s] \
                            %(funcName)s: %(message)s"</span>,
    level<span class="hljs-operator">=</span>logging.DEBUG,
  )
logger <span class="hljs-operator">=</span> logging.getLogger()
</code></pre><p>this log configuration sets the minimum logging level to (Ill explain logging in another article) and every time we log a message to the terminal, it sets it to automatically log the time (asctime) the log is created, the level of the log (levelname), the name of the function (funcName) and the message of the log.
Next, well set the destination where the file is located</p>
<pre><code># <span class="hljs-keyword">Location</span> <span class="hljs-keyword">to</span> the <span class="hljs-type">json</span> file
<span class="hljs-keyword">LOCATION</span> = "../../data/raw/Clothing_Shoes_and_Jewelry_5.json"
</code></pre><p>Well define functions to get read each line of the dataset, get all description keys</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_json</span>(<span class="hljs-params">line</span>):</span>
    <span class="hljs-string">"""This function reads a line of the json file as a dictionary"""</span>
    <span class="hljs-keyword">return</span> json.loads(line)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_desc_keys</span>(<span class="hljs-params">line</span>):</span>
    <span class="hljs-string">"""This function gets the description keys in each line"""</span>
    KEY = <span class="hljs-string">"style"</span>
    desc_dict = line.get(KEY)
    <span class="hljs-keyword">if</span> desc_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> list(desc_dict.keys())
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>
</code></pre><p>The first function read_json takes in a line of data file after it has been opened with python open() and returns a dictionary containing the content. Usually, after opening the file, you load the entire file with json.loads to get the entire data as a list of dictionaries but because we risk running out of memory that way, thus we are iterating over each line of the data file, therefore by default, python is reading the line as a string before we covert to a dictionary using json.loads but the line only and not the entire dataset.
The second function get_desc_keys take in the line of the file (now a dictionary) and looks for the style inside it (because some products dont have it at all). If the style key is found, it retrieves all the keys in the dictionary (since the value of the style key is a dictionary, for which each key is a descriptor of the file). If the style key is absent, it returns None (which means nothing).
Next, before we continue, we want to obtain all the description keys in the entire dataset.</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_desc_keys</span>(<span class="hljs-params">LOCATION</span>):</span>
    <span class="hljs-string">"""
    This function combines all the descriptions of each data entry to obtain
    all unique descriptions in the 'style' key

    THIS FUNCTION IS TO RUN JUST ONCE
    """</span>
    file = open(LOCATION, <span class="hljs-string">"rb"</span>)
    desc = list()
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(file, desc=<span class="hljs-string">"Getting all the descriptions"</span>):
        json_dict = read_json(line)
        line_desc = get_desc_keys(json_dict)
        <span class="hljs-keyword">if</span> line_desc <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            desc.extend(line_desc)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">pass</span>
    logger.info(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>all product desc keys obtained successfuly<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>"</span>)
    <span class="hljs-keyword">return</span> list(set(desc))
</code></pre><p>First, we open the file to read, and then we create a desc list to host all of the description keys we find. Next, we iterate through each line of the file, tqdm gives us a progress bar to see how far the progress is, with our function, we then use the read_json that we created earlier to convert the line to dictionary while the get_desc_keys to get the description keys in each line and we add the list returned to the existing desc list that contains all the desc keys.
After the entire file has been read, we then log a message that all the product desc keys have been retrieved, and then we return a list containing all unique values in the list.
Next, well create a list containing all the original keys in every line and the descriptors in the style key and also remove the style key since we can directly get values in them.</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_columns</span>(<span class="hljs-params">json_line, descriptions</span>):</span>
    <span class="hljs-string">"""
    This function gets the keys (to be used as columns) and the description
    keys and combines all of them together as the columns to be used
    """</span>
    column_keys = list(json_line.keys())
    column_keys.extend(descriptions)
    column_keys.remove(<span class="hljs-string">"style"</span>)
    logger.info(<span class="hljs-string">"Columns in the file obtaained successfuly"</span>)
    <span class="hljs-keyword">return</span> column_keys
</code></pre><p>The function obtains the keys (columns to be) in the line (dictionary produced by read_json and combines the list returned get_all_desc_keys (descriptions) and logs that the task has been completed.
All we have been doing previously is get a compilation of what well define some functions to get what columns will be in our new CSV and the keys well use to get the values, next, well get the actual values.
But before we get the actual values, we want to get only the lines with a style key, so well create a function to check that and return a boolean (True or False), if False, it should log that the product desc is absent.</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PRODUCT_DESC_PRESENT</span>(<span class="hljs-params">line_dict</span>):</span>
    <span class="hljs-string">"""
    This function evaluates if product description (key 'style')
    exists and return bool yes or no
    """</span>
    STYLE = <span class="hljs-string">"style"</span>
    desc = line_dict.get(STYLE)
    <span class="hljs-keyword">if</span> desc <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># logger.info("product desc keys absent thus passing....")</span>
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
</code></pre><p>Next, we get the actual values.</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">strip_values</span>(<span class="hljs-params">value</span>):</span>
    <span class="hljs-string">"""This function strips the strings or return the original value"""</span>
    <span class="hljs-keyword">if</span> type(value) <span class="hljs-keyword">is</span> str:
        <span class="hljs-keyword">return</span> value.strip()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> value

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_values</span>(<span class="hljs-params">prod_dict, columns, all_prod_desc</span>):</span>
    <span class="hljs-string">"""
    This function gets the values from the keys of the json file present
    """</span>
    new_prod_dict = dict()
    STYLE = <span class="hljs-string">"style"</span>
    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> columns:
        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> all_prod_desc:
            new_prod_dict[key] = strip_values(prod_dict.get(key))
        <span class="hljs-keyword">elif</span> key <span class="hljs-keyword">in</span> all_prod_desc:
            value = prod_dict[STYLE].get(key)
            <span class="hljs-keyword">if</span> value <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                new_prod_dict[key] = <span class="hljs-string">"null"</span>
            <span class="hljs-keyword">else</span>:
                new_prod_dict[key] = strip_values(value)
    <span class="hljs-keyword">return</span> new_prod_dict
</code></pre><p>The strip_values check is the value passed into it is a string and removes any trailing or preceding blank space, so well have a compact value
The get_values function receives a line of the document (returned by read_json), all the columns (returned by get_columns) and the all product description keys (returned by all_prod_desc_keys). 
It then iterates through all values in the columns to check if the value is in the all_product_desc, if it is not, it retrieves it directly, if it is in the all_prod_desc, it retrieves the value the from the style key. If the style key doesnt have the column, then it returns None and we assign null to the value [You can assign this anything]. It then returns back a full dictionary with all the columns assigned a value.
Now, we are ready to start creating our new CSV files, well create a couple of functions to help with this </p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_file_destination</span>(<span class="hljs-params">LOCATION</span>):</span>
    <span class="hljs-string">"""
    This function sets the destination to save the files to as 'data/processed'
    """</span>
    destination_list = LOCATION.split(<span class="hljs-string">"/"</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-string">"/"</span>.join(destination_list[:<span class="hljs-number">-2</span>]) + str(<span class="hljs-string">"/processed/"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_file</span>(<span class="hljs-params">file_destination, n_files</span>):</span>
    <span class="hljs-string">"""This function creates a new file where the csv file will be saved"""</span>
    filename = file_destination + <span class="hljs-string">"FILE_"</span> + str(n_files) + <span class="hljs-string">".csv"</span>
    file = open(filename, <span class="hljs-string">"w"</span>, newline=<span class="hljs-string">""</span>)
    n_files += <span class="hljs-number">1</span>
    logger.info(<span class="hljs-string">f"file <span class="hljs-subst">{filename}</span> created successfuly"</span>)
    <span class="hljs-keyword">return</span> file, n_files
</code></pre><p>The get_file_destination gets the new directory where we want to save our new files. The existing dataset is in the /data/raw folder, whereas we want to save the file in the /data/processed/ folder. This part is absolutely optional, use only if you want to create the files in a different folder.
The create_file creates a new file in the file_destination passed to it, it adds a number to the file name (therefore if n_files passed to it is 0, it creates FILE_0.csv at the file destination and returns the file (without closing yet)
and the n_files+1.
Next, we want to create optional checks in our file if we have written 500,000 lines to the new file. We want to save our files when they have 500,000 lines already and create another line [This part is totally optional]</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">NEW_FILE_BREAK</span>(<span class="hljs-params">n_rows</span>):</span>
    <span class="hljs-string">"""
    This function evaluates if we have 500,000 entries in the existing file
    """</span>
    expected_break = <span class="hljs-number">500000</span>
    <span class="hljs-keyword">if</span> n_rows % expected_break == <span class="hljs-number">0</span>:
        logger.info(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>500,000 lines written successfuly<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
</code></pre><p>After this, we then use csv.Dictwriter to write a new line to our file, the CSV writer will be instantiated in the next function, but this function receives the CVS writer and the values well like to pass</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">write_line</span>(<span class="hljs-params">csv_writer, values</span>):</span>
    <span class="hljs-string">"""
    This function writes a new line into the CSV file containing all our data
    """</span>
    <span class="hljs-keyword">return</span> csv_writer.writerow(values)
</code></pre><p>In this final function, well combine all the functions we have written and implement some minor functions we havent defined before to carry out our task</p>
<pre><code>def main():
    n_rows <span class="hljs-operator">=</span> <span class="hljs-number">1</span>
    n_files. = <span class="hljs-number">1</span>
    columns <span class="hljs-operator">=</span> list()
    all_prod_desc <span class="hljs-operator">=</span> get_all_desc_keys(LOCATION)
    with open(LOCATION, <span class="hljs-string">"rb"</span>) <span class="hljs-keyword">as</span> file:
         file_destination <span class="hljs-operator">=</span> get_file_destination(LOCATION)
         processed_file, n_files <span class="hljs-operator">=</span> create_file(file_destination, n_files)
         csv_writer <span class="hljs-operator">=</span> None
         <span class="hljs-keyword">for</span> line in tqdm(file):
            line_dict <span class="hljs-operator">=</span> read_json.run(line)
            <span class="hljs-keyword">if</span> n_rows <span class="hljs-operator">=</span><span class="hljs-operator">=</span> <span class="hljs-number">1</span>:
                columns <span class="hljs-operator">=</span> get_columns(line_dict, all_prod_desc)
                csv_writer <span class="hljs-operator">=</span> csv.DictWriter(processed_file, fieldnames<span class="hljs-operator">=</span>columns)
                csv_writer.writeheader()
            <span class="hljs-keyword">else</span>:
                pass
            <span class="hljs-keyword">if</span> PRODUCT_DESC_PRESENT(line_dict):
               n_rows <span class="hljs-operator">+</span><span class="hljs-operator">=</span> <span class="hljs-number">1</span>
               values <span class="hljs-operator">=</span> get_values(line_dict, columns, all_prod_desc)
               <span class="hljs-keyword">if</span> NEW_FILE_BREAK(n_rows):
                    processed_file.close()
                     processed_file, n_files <span class="hljs-operator">=</span> create_file(file_destination, n_files)
                     csv_writer <span class="hljs-operator">=</span> csv.DictWriter(processed_file, fieldnames<span class="hljs-operator">=</span>columns)
                     csv_writer.writeheader()
                <span class="hljs-keyword">else</span>:
                    pass
                write_line.run(csv_writer, values)
            <span class="hljs-keyword">else</span>:
                pass
        processed_file.close()
        logger.info(f<span class="hljs-string">"{'-'*20}Task complete, {n_rows} written....{'-'*20}"</span>)
    <span class="hljs-keyword">return</span> main
</code></pre><p>We start the function by assigning some variables, n_rows, n_file and columns. We then run the get_all_desc_keys function and assigned it to all_desc_keys. We then opened the original dataset with context manager and create our first empty CSV file. We instantiated the csv_writer initially to None and began to loop through our file and read the line to a dictionary (with read_json). For the first line, we obtained all the columns (with all_desc_keys already retrieved).
We then created the csv.DictWriter, csv.DictWriter retrieves values from our dictionary (to be created by the get_values) and writes them as a CSV line. [This is because the usual CSV writer (csv.writer) expects a list and writes it in CSV format into the file, csv.DictWriter enables us to return a dictionary instead and write it in CSV format].
Next, we write the headers (list of all the columns), then we check if the line has a style key, if present, we retrieve the values (in a dictionary). After this, well check if we have written 500,000 lines into the currently open file, we track this by the n_rows, if yes, we create close the existing file, create another one and instantiate the csv.DictWriter again and write headers, if we havent written 500,000 lines into the open file yet, then we write the values of that line into the open file and add one to n_rows. Once we are done with all these, we close the last open file and voila, that is it.</p>
<pre><code>if <span class="hljs-strong">__name__</span> == <span class="hljs-strong">__main__</span>:
<span class="hljs-code">    main()</span>
</code></pre><div class="embed-wrapper"><div class="embed-loading"><div class="loadingRow"></div><div class="loadingRow"></div></div><a class="embed-card" href="https://media.giphy.com/media/fRge2RHoFX8QFeQ6R0/giphy.gif">https://media.giphy.com/media/fRge2RHoFX8QFeQ6R0/giphy.gif</a></div>
<h2 id="heading-where-does-prefect-come-in">Where does prefect come in?</h2>
<p>I mean, we have written the pipeline without actually using prefect, so where do we use it?
Prefect is actually a python workflow manager, it is a data engineering tool that ensures your code runs well and when it fails, it guarantees that it fails successfully (this means you know exactly where it failed). It is based on Directed Acrylic Graph (DAG) which in simple terms are python functions tied and directed towards another to form a pipeline (in a similar way as weve done). It also gives you the ability to schedule when you want your pipelines to run, what to do when it fails and how many times to retry.
Prefect has 4 major units, namely task, flow, scheduler and parameters but well be using majorly two, task and flow. Task enables us to convert our function to a single step in our workflow. To use task, we just need to add @task as a decorator to our functions.
decorators are python functions which take in another function, e.g our function in this case, so as to carry out the function we have described and some other things in addition to our function.
For all functions encapsulated in prefect tasks, we need to use them either with prefect Flow context manager (the second prefect tool well be using) or use .run() method with our functions when we want to call them (to be on the safe side, well use both).
The other prefect tools are parameters, which enable us to pass in different parameters we want to try with our pipeline and prefect scheduler, we enable us to schedule our pipeline to run at any time we specify, since we just want to run our pipeline once, we wont be using this. So Ill reinplement all of our function with prefect flow</p>
<pre><code><span class="hljs-comment"># Loading necessary libraries</span>
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">from</span> prefect <span class="hljs-keyword">import</span> task, Flow
<span class="hljs-keyword">import</span> logging

logging.basicConfig(
    format=<span class="hljs-string">"%(asctime)s [%(levelname)s] \
                            %(funcName)s: %(message)s"</span>,
    level=logging.DEBUG,
)
logger = logging.getLogger()

<span class="hljs-comment"># Location to the json file</span>
LOCATION = <span class="hljs-string">"../../data/raw/Clothing_Shoes_and_Jewelry_5.json"</span> 

<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_json</span>(<span class="hljs-params">line</span>):</span>
    <span class="hljs-string">"""This function reads a line of the json file as a dictionary"""</span>
    <span class="hljs-keyword">return</span> json.loads(line)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_desc_keys</span>(<span class="hljs-params">line</span>):</span>
    <span class="hljs-string">"""This function gets the description keys in each line"""</span>
    KEY = <span class="hljs-string">"style"</span>
    desc_dict = line.get(KEY)
    <span class="hljs-keyword">if</span> desc_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> list(desc_dict.keys())
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>

<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_desc_keys</span>(<span class="hljs-params">LOCATION</span>):</span>
    <span class="hljs-string">"""
    This function combines all the descriptions of each data entry to obtain
    all unique descriptions in the 'style' key

    THIS FUNCTION IS TO RUN JUST ONCE
    """</span>
    file = open(LOCATION, <span class="hljs-string">"rb"</span>)
    desc = list()
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(file, desc=<span class="hljs-string">"Getting all the descriptions"</span>):
        json_dict = read_json.run(line)
        line_desc = get_desc_keys(json_dict)
        <span class="hljs-keyword">if</span> line_desc <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:

 desc.extend(line_desc)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">pass</span>
logger.info(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>all product desc keys obtained successfully<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>"</span>)
    <span class="hljs-keyword">return</span> list(set(desc))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_columns</span>(<span class="hljs-params">json_line, descriptions</span>):</span>
    <span class="hljs-string">"""
    This function gets the keys (to be used as columns) and the description
    keys and combines all of them together as the columns to be used
    """</span>
    column_keys = list(json_line.keys())
    column_keys.extend(descriptions)
    column_keys.remove(<span class="hljs-string">"style"</span>)
    logger.info(<span class="hljs-string">"Columns in the file obtained successfuly"</span>)
    <span class="hljs-keyword">return</span> column_keys

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PRODUCT_DESC_PRESENT</span>(<span class="hljs-params">line_dict</span>):</span>
    <span class="hljs-string">"""
    This function evaluates if product description (key 'style')
    exists and return bool yes or no
    """</span>
    STYLE = <span class="hljs-string">"style"</span>
    desc = line_dict.get(STYLE)
    <span class="hljs-keyword">if</span> desc <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># logger.info("product desc keys absent thus passing....")</span>
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">strip_values</span>(<span class="hljs-params">value</span>):</span>
    <span class="hljs-string">"""This function strips the strings or return the original value"""</span>
    <span class="hljs-keyword">if</span> type(value) <span class="hljs-keyword">is</span> str:
        <span class="hljs-keyword">return</span> value.strip()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> value

<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_values</span>(<span class="hljs-params">prod_dict, columns, all_prod_desc</span>):</span>
    <span class="hljs-string">"""
    This function gets the values from the keys of the json file present
    """</span>
    new_prod_dict = dict()
    STYLE = <span class="hljs-string">"style"</span>
    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> columns:
        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> all_prod_desc:
            new_prod_dict[key] = strip_values(prod_dict.get(key))
        <span class="hljs-keyword">elif</span> key <span class="hljs-keyword">in</span> all_prod_desc:
            value = prod_dict[STYLE].get(key)
            <span class="hljs-keyword">if</span> value <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                new_prod_dict[key] = <span class="hljs-string">"null"</span>
            <span class="hljs-keyword">else</span>:
                new_prod_dict[key] = strip_values(value)
    <span class="hljs-keyword">return</span> new_prod_dict


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_file_destination</span>(<span class="hljs-params">LOCATION</span>):</span>
    <span class="hljs-string">"""
    This function sets the destination to save the files to as 'data/processed'
    """</span>
    destination_list = LOCATION.split(<span class="hljs-string">"/"</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-string">"/"</span>.join(destination_list[:<span class="hljs-number">-2</span>]) + str(<span class="hljs-string">"/processed/"</span>)

<span class="hljs-meta">@task(nout=2)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_file</span>(<span class="hljs-params">file_destination, n_files</span>):</span>
    <span class="hljs-string">"""This function creates a new file where the csv file will be saved"""</span>
    filename = file_destination + <span class="hljs-string">"FILE_"</span> + str(n_files) + <span class="hljs-string">".csv"</span>
    file = open(filename, <span class="hljs-string">"w"</span>, newline=<span class="hljs-string">""</span>)
    n_files += <span class="hljs-number">1</span>
    logger.info(<span class="hljs-string">f"file <span class="hljs-subst">{filename}</span> created successfuly"</span>)
    <span class="hljs-keyword">return</span> file, n_files

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">NEW_FILE_BREAK</span>(<span class="hljs-params">n_rows</span>):</span>
    <span class="hljs-string">"""
    This function evaluates if we have 500,000 entries in the existing file
    """</span>
    expected_break = <span class="hljs-number">500000</span>
    <span class="hljs-keyword">if</span> n_rows % expected_break == <span class="hljs-number">0</span>:
        logger.info(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>500,000 lines written successfuly<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>

<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">write_line</span>(<span class="hljs-params">csv_writer, values</span>):</span>
    <span class="hljs-string">"""
    This function writes a new line into the csv file containing all our data
    """</span>
    <span class="hljs-keyword">return</span> csv_writer.writerow(values)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span>
    <span class="hljs-keyword">with</span> Flow(name=<span class="hljs-string">"json_to_csv_etl"</span>) <span class="hljs-keyword">as</span> flow:
        n_rows = <span class="hljs-number">1</span>
        n_files = <span class="hljs-number">1</span>
        columns = list()
        all_prod_desc = get_all_desc_keys.run(LOCATION)
        <span class="hljs-keyword">with</span> open(LOCATION, <span class="hljs-string">"rb"</span>) <span class="hljs-keyword">as</span> file:
            file_destination = get_file_destination(LOCATION)
            processed_file, n_files = create_file.run(file_destination, n_files)
            csv_writer = <span class="hljs-literal">None</span>
            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(file):
                line_dict = read_json.run(line)
                <span class="hljs-keyword">if</span> n_rows == <span class="hljs-number">1</span>:
                    columns = get_columns(line_dict, all_prod_desc)
                    csv_writer = csv.DictWriter(processed_file, fieldnames=columns)
                    csv_writer.writeheader()
                <span class="hljs-keyword">else</span>:
                    <span class="hljs-keyword">pass</span>
                <span class="hljs-keyword">if</span> PRODUCT_DESC_PRESENT(line_dict):
                    n_rows += <span class="hljs-number">1</span>
                    values = get_values.run(line_dict, columns, all_prod_desc)
                    <span class="hljs-keyword">if</span> NEW_FILE_BREAK(n_rows):
                        processed_file.close()
                        processed_file, n_files = create_file.run(
                            file_destination, n_files
                        )
                        csv_writer = csv.DictWriter(processed_file, fieldnames=columns)
                        csv_writer.writeheader()
                    <span class="hljs-keyword">else</span>:
                        <span class="hljs-keyword">pass</span>
                    write_line.run(csv_writer, values)
                <span class="hljs-keyword">else</span>:
                    <span class="hljs-keyword">pass</span>
            processed_file.close()
            logger.info(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>Task complete, <span class="hljs-subst">{n_rows}</span> written....<span class="hljs-subst">{<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>}</span>"</span>)
    <span class="hljs-keyword">return</span> flow

  <span class="hljs-keyword">if</span> __name__ == __main__:
      Flow = main()
      Flow.run()
</code></pre><p>In this new implementation, I added prefect task in bold. One last important thing to note, for tasks that return more than one value, the number of outputs is expected to be specified using arguments nout or converted to a tuple. This is why the create_file task has nout set to 2 because it returns two values.
And this is the end of this tutorial, if you enjoyed it, please drop positive feedback, follow and share. You can also follow me on twitter <a target="_blank" href="https://www.twitter.com/madeofajala">@madeofajala</a> thank you and see you later🏾.</p>
</p>
      <p class="post-meta">
        1 min read &nbsp; &middot; &nbsp;
        June 19, 2022
        &nbsp; &middot; &nbsp; hashnode.com
      </p>
      <p class="post-tags">
        <a href="/blog/2022">
          <i class="fas fa-calendar fa-sm"></i> 2022 </a>

          

          
    </p></li>

    

    
    
    
    

    <li><h3>
        
          <a class="post-title" href="/assets/pdf/example_pdf.pdf">a post with redirect</a>
        
      </h3>
      <p>you can also redirect to assets like pdf</p>
      <p class="post-meta">
        1 min read &nbsp; &middot; &nbsp;
        February 1, 2022
      </p>
      <p class="post-tags">
        <a href="/blog/2022">
          <i class="fas fa-calendar fa-sm"></i> 2022 </a>

          

          
    </p></li>

    
  </ul>

  <nav aria-label="Blog page naviation">
  <ul class="pagination pagination-lg justify-content-center">
    <li class="page-item ">
      <a class="page-link" href="/blog/page/2/" tabindex="-1" aria-disabled="2">Newer</a>
    </li><li class="page-item "><a class="page-link" href="/blog/page/2/index.html" title="blog - page 2">2</a></li>
      <li class="page-item active"><a class="page-link" href="/blog/page/3/index.html" title="blog - page 3">3</a></li>
      <li class="page-item "><a class="page-link" href="/blog/page/4/index.html" title="blog - page 4">4</a></li>
      <li class="page-item "><a class="page-link" href="/blog/page/5/index.html" title="blog - page 5">5</a></li>
      <li class="page-item "><a class="page-link" href="/blog/page/6/index.html" title="blog - page 6">6</a></li>
      <li class="page-item ">
      <a class="page-link" href="/blog/page/4/">Older</a>
    </li>
  </ul>
</nav>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2023 Marvellous Oluwamayowa Ajala. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
